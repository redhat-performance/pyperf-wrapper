#!/bin/bash
#    An automated script to run pyperformane using specified python binaries
#    Copyright (C) 2024 David Valin <dvalin@redhat.com>
#
#    This program is free software; you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation; either version 2 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License along
#    with this program; if not, see <https://www.gnu.org/licenses/>.

PATH="${PATH}:/usr/local/bin"
export PATH
python_pkgs=""
python_exec=""
PYPERF_VERSION="1.11.0"
number_pyperfs=1
pindex=0
pid_index=0
declare -a pids
pyresults=""

#
# To make sure.
#
exit_out()
{
	echo $1
	exit $2
}

usage()
{
	echo "$1 Usage:"
	echo "--procs <x>: Number of pyperfs to execute at the same time, default is 1"
	echo "--pyperf_version <version number>: Version of pyperf to run, default is $PYPERF_VERSION."
	echo "--python_exec_path: Python to set via alternatives"
	echo "--python_pkgs: comma seprated list of python packages to install"
	source test_tools/general_setup --usage
	exit 1
}

convert_unit_to_ns()
{
	unit=$1
	base=1

	if [[ $unit == "ms" ]]; then
		base=`echo 1000*1000 | bc`
	elif [[ $unit == "us" ]]; then
		base=1000
	elif [[ $unit == "ns" ]]; then
		base=1
	else
		echo unknown unit $unit
		exit 1
	fi
	echo $base
}

record_value()
{
	file=$1
	sum=$2
	test=$3
	unit=ns
	#
	# Convert time period to something managable.
	#
	full=`echo $sum | cut -d. -f 1`
	if [ $full -gt 1000000 ]; then
		unit=ms
		full=`echo $full/1000000 | bc`
	elif [ $full -gt 1000 ]; then
		unit=us
		full=`echo $full/1000 | bc`
	fi
	echo $test:$full:$unit >> $file
}

combine_csv_files()
{
	data=0
	prefix=$1
	first_file=${1}_1.csv
	file_list=`ls ${prefix}*csv`
	if [[ $first_file == $file_list ]]; then
		#
		# Nothing to combine, just the one file.
		#
		cp $first_file ${prefix}.csv
		cp $first_file ${prefix}_avg.csv
	else
		#
		# Need to merge the files.
		#
		while IFS= read -r line
		do
			if [ $data -eq 0 ]; then
				echo $line >> $1.csv
				if [[ $line == "Test:"* ]]; then
					data=1
				fi
				continue
			fi
			test=`echo $line | cut -d':' -f 1`
			test_res=`grep "${test}:" ${prefix}*csv | cut -d':' -f2-`
			numb_vals=0
			sum=0
			for i in $test_res; do
				value=`echo $i | cut -d':' -f2`
				unit=`echo $i | cut -d':' -f 3`
				cnvt=$(convert_unit_to_ns $unit)
				value=`echo $value*$cnvt | bc`
				sum=`echo "$sum+$value" | bc`
				let "numb_vals=$numb_vals+1"
			done
			record_value $1.csv $sum $test
			avg=`echo $sum/$numb_vals | bc | cut -d'.' -f 1`
			record_value $1_avg.csv $avg $test
		done < "${first_file}"
	fi
}

install_tools()
{
	show_usage=0
	#
	# Clone the repo that contains the common code and tools
	#
	tools_git=https://github.com/redhat-performance/test_tools-wrappers

	found=0
	for arg in "$@"; do
		if [ $found -eq 1 ]; then
			tools_git=$arg
			found=0
		fi
		if [[ $arg == "--tools_git" ]]; then
			found=1
		fi
		#
		# We do the usage check here, as we do not want to be calling
		# the common parsers then checking for usage here.  Doing so will
		# result in the script exiting with out giving the test options.
		#
		if [[ $arg == "--usage" ]]; then
			show_usage=1
		fi
	done

	#
	# Check to see if the test tools directory exists.  If it does, we do not need to
	# clone the repo.
	#
	if [ ! -d "test_tools" ]; then
		git clone $tools_git test_tools
		if [ $? -ne 0 ]; then
			exit_out "pulling git $tools_git failed." 1
		fi
	fi

	if [ $show_usage -eq 1 ]; then
		usage $1
	fi
}

generate_csv_file()
{
	instance=0
	float=0
	ivalue=0
	fvalue=0.0
	test_name=""
	unit=""
	reduce=0
	res_count=0
	value_sum=0
	start=0

	$TOOLS_BIN/test_header_info --front_matter --results_file "${1}.csv" --host $to_configuration --sys_type $to_sys_type --tuned $to_tuned_setting --results_version $PYPERF_VERSION --test_name $test_name_run
	echo "Test:Avg:Unit" >> "${1}.csv"

	while IFS= read -r line
	do
		if [[ $test_name == "" ]]; then
			test_name=$line
			continue
		fi
		if [ $start -eq 0 ]; then
			if [[ $line == "###"* ]]; then
				start=1;
			fi
		fi
		if [ $start -eq 0 ]; then
			continue
		fi
		if [[ $line == "" ]]; then
			continue
		fi
		if [[ $line == "###"* ]]; then
			test_name=`echo $line | cut -d' ' -f2`
		else
			value=`echo $line | cut -d' ' -f 5`
			unit=`echo $line | cut -d' ' -f 6`
			echo $test_name:$value:$unit >> "${1}.csv"
		fi
	done < "${1}.results"
}

pip3_install()
{
	if [ $to_no_pkg_install -eq 0 ]; then
		pip3 -q install $1
		if [ $? -ne 0 ]; then
			exit_out "pip3 install of $1 failed." 1
		fi
	fi
}
#
# Variables set by general setup.
#
# TOOLS_BIN: points to the tool directory
# to_home_root: home directory
# to_configuration: configuration information
# to_times_to_run: number of times to run the test
# to_pbench: Run the test via pbench
# to_pbench_copy: Copy the data to the pbench repository, not move_it.
# to_puser: User running pbench
# to_run_label: Label for the run
# to_user: User on the test system running the test
# to_sys_type: for results info, basically aws, azure or local
# to_sysname: name of the system
# to_tuned_setting: tuned setting
#

install_tools $0

test_name_run="pyperf"
arguments="$@"

curdir=`pwd`

if [[ $0 == "./"* ]]; then
	chars=`echo $0 | awk -v RS='/' 'END{print NR-1}'`
	if [[ $chars == 1 ]]; then
		run_dir=`pwd`
	else
		run_dir=`echo $0 | cut -d'/' -f 1-${chars} | cut -d'.' -f2-`
		run_dir="${curdir}${run_dir}"
	fi
elif [[ $0 != "/"* ]]; then
	dir=`echo $0 | rev | cut -d'/' -f2- | rev`
	run_dir="${curdir}/${dir}"
else
	chars=`echo $0 | awk -v RS='/' 'END{print NR-1}'`
	run_dir=`echo $0 | cut -d'/' -f 1-${chars}`
	if [[ $run_dir != "/"* ]]; then
		run_dir=${curdir}/${run_dir}
	fi
fi
# Gather hardware information
${curdir}/test_tools/gather_data ${curdir}


if [ ! -f "/tmp/pyperf.out" ]; thencommand="${0} $@"
	command="${0} $@"
	$command &> /tmp/pyperf.out
	rtc=$?
	if [ -f /tmp/pyperf.out ]; then
		echo =================================
		echo Output from the test.
		echo =================================
		cat /tmp/pyperf.out
		rm /tmp/pyperf.out
	fi
	exit $rtc
fi


if [ -d "workloads" ]; then
	#
	# If running from zathras, workloads will be symlinked to
	# to /mnt.  Which is done due to azure having a very small
	# user space.
	#
	start_dir=`pwd`
	cd workloads
	for file in `ls ${start_dir}`; do
		if [[ ! -f $file ]] && [[ ! -d $file ]]; then
			ln -s $start_dir/$file .
		fi
	done
fi

source test_tools/general_setup "$@"

ARGUMENT_LIST=(
	"procs"
	"pyperf_version"
	"python_exec"
	"python_pkgs"
)

NO_ARGUMENTS=(
	"usage"
)

# read arguments
opts=$(getopt \
	--longoptions "$(printf "%s:," "${ARGUMENT_LIST[@]}")" \
	--longoptions "$(printf "%s," "${NO_ARGUMENTS[@]}")" \
	--name "$(basename "$0")" \
	--options "h" \
	-- "$@"
)

eval set --$opts

while [[ $# -gt 0 ]]; do
	case "$1" in
		--procs)
			number_pyperfs=$2
			shift 2
		;;
		--pyperf_version)
			PYPERF_VERSION=$2
			shift 2
		;;
		--python_exec)
			python_exec=$2
			shift 2
		;;
		--python_pkgs)
			python_pkgs=$2
			shift 2
		;;
		--usage)
			usage $0
		;;
		-h)
			usage $0
		;;
		--)
			break
		;;
		*)
			echo option not found $1
			usage $0
		;;
	esac
done

if [ $to_pbench -eq 0 ]; then
	rm -rf pyperformance
	#
	# Install pip/pip3
	#
	if [[ $python_exec != "" ]]; then
		if [[ ! -f $python_exec ]]; then
			exit_out "Error: Designated python executable, $python_exec, not present"
		fi
		#
		# Remove the existing (if any) default python.
		#
		alternatives --remove-all python
 		alternatives --install /usr/bin/python python $python_exec 1
	fi

	pid_index=0;
	cpus=`cat /proc/cpuinfo | grep processor | wc -l`
	cous=1
	mkdir -p pyperf_results
	
	pyresults=pyperf_results/pyperf_out_$(date "+%Y.%m.%d-%H.%M.%S")
	export VIRTUAL_ENV_DISABLE_PROMPT=1
	while [[ "$pid_index" != "$number_pyperfs" ]];
	do
		pindex=$pid_index
		let "pid_index=$pid_index+1"
		$run_dir/run_pyperf ${pyresults}_${pid_index} &
		pids[${pindex}]=$!
	done
	for pid in ${pids[*]}; do
		wait $pid
		if [ $? -ne 0 ]; then
			echo "Failed: python3 -m pyperformance run --output  ${pyresults}"
		fi
	done
	rm -rf ${pyresults}*env
	pid_index=0
	while [[ $pid_index < $number_pyperfs ]];
	do
		let "pid_index=$pid_index+1"
		generate_csv_file ${pyresults}_${pid_index}
	done
	combine_csv_files ${pyresults}
else
	source ~/.bashrc
	arguments="${arguments} --test_iterations ${to_times_to_run}"
	cd $curdir
	echo $TOOLS_BIN/execute_via_pbench --cmd_executing "$0" $arguments --test ${test_name_run} --spacing 11 --pbench_stats $to_pstats
	$TOOLS_BIN/execute_via_pbench --cmd_executing "$0" $arguments --test ${test_name_run} --spacing 11 --pbench_stats $to_pstats
	if [ $? -ne 0  ]; then
		exit_out "Failed: $TOOLS_BIN/execute_via_pbench --cmd_executing "$0" $arguments --test ${test_name_run} --spacing 11 --pbench_stats $to_pstats"
	fi
	exit 0
fi


#
# Process the data.
#
${curdir}/test_tools/save_results --curdir $curdir --home_root $to_home_root --results /tmp/pyperf.out  --test_name pyperf --tuned_setting=$to_tuned_setting --version NONE --user $to_user --other_files "$pyresults*,test_results_report"
exit 0
