#!/bin/bash
#    An automated script to run pyperformane using specified python binaries
#    Copyright (C) 2024 David Valin <dvalin@redhat.com>
#
#    This program is free software; you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation; either version 2 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License along
#    with this program; if not, see <https://www.gnu.org/licenses/>.

PATH="${PATH}:/usr/local/bin"
export PATH
python_pkgs=""
python_exec="python3"
install_pip=0
PYTHON_VERSION=""
PYPERF_VERSION="1.11.0"
copy_dirs=""

base_dir=$(dirname $(realpath $0))
#
# To make sure.
#
exit_out()
{
	if [[ $to_use_pcp -eq 1 ]]; then
		stop_pcp_subset
		stop_pcp
		shutdown_pcp
	fi

	echo $1
	exit $2
}

usage()
{
	echo "$1 Usage:"
	echo "--pyperf_version <version number>: Version of pyperf to run, default is $PYPERF_VERSION."
	echo "--python_exec_path: Python to set via alternatives"
	echo "--python_pkgs: comma seprated list of python packages to install"
	source test_tools/general_setup --usage
        exit 1
}

install_tools()
{
	show_usage=0
	#
	# Clone the repo that contains the common code and tools
	#
	tools_git=https://github.com/redhat-performance/test_tools-wrappers

	found=0
	for arg in "$@"; do
		if [ $found -eq 1 ]; then
			tools_git=$arg
			found=0
		fi
		if [[ $arg == "--tools_git" ]]; then
			found=1
		fi
		#
		# We do the usage check here, as we do not want to be calling
		# the common parsers then checking for usage here.  Doing so will
		# result in the script exiting with out giving the test options.
		#
		if [[ $arg == "--usage" ]]; then
			show_usage=1
		fi
	done

	#
	# Check to see if the test tools directory exists.  If it does, we do not need to
	# clone the repo.
	#
	if [ ! -d "test_tools" ]; then
		git clone $tools_git test_tools
		if [ $? -ne 0 ]; then
			exit_out "pulling git $tools_git failed." 1
		fi
	fi

	if [ $show_usage -eq 1 ]; then
		usage $1
	fi
}

generate_csv_file()
{
	instance=0
	float=0
	ivalue=0
	fvalue=0.0
	test_name=""
	unit=""
	reduce=0
	res_count=0
	value_sum=0

  $TOOLS_BIN/test_header_info --front_matter --results_file "${1}.csv" --host $to_configuration --sys_type $to_sys_type --tuned $to_tuned_setting --results_version "py${PYTHON_VERSION}_$PYPERF_VERSION" --test_name $test_name_run 

	echo "Test:Avg:Unit" >> "${1}.csv"
	while IFS= read -r line
	do
		if [[ $test_name == "" ]]; then
			test_name=$line
			continue
		fi
		if [ -z "$line" ]; then
			let "reduce=$reduce+1"
			if [[ $reduce -eq 2 ]]; then
				if [[ $test_name != *"WARNING:"* ]]; then
					results=`echo "${value_sum}/${res_count}" | bc -l`
					printf "%s:%.2f:%s\n" $test_name $results $unit >> ${1}.csv
					
					if [[ $to_use_pcp -eq 1 ]]; then
						metric_name="pyperf_${test_name}"
						ns_results=$($TOOLS_BIN/convert_val --from_unit $unit --to_unit ns --time_val --value $results | sed -e 's/ns//g')
						# Cannot use convert_val directly since it does not support floating point output
						sec_results=$(echo "$ns_results/1000000000" | bc -l)
						result2pcp "$metric_name" "${sec_results}"
					fi
				fi
				reduce=0
				res_count=0
				value_sum=0
				test_name=""
			fi
			continue
		fi
		if [[ $line == *"--"* ]] || [[ $line == *"calibrate"* ]] || [[ $line == *"warmup"* ]] || [[ $line != *"value"* ]]; then
			continue
		fi
		value=`echo $line | cut -d' ' -f 4`
		unit=`echo $line | cut -d' ' -f 5`
		let "res_count=${res_count}+1"
		value_sum=`echo "${value}+${value_sum}" | bc -l`
	done < "${1}.results"
	results=`echo "${value_sum}/${res_count}" | bc -l`
	printf "%s:%.2f:%s\n" $test_name $results $unit >> ${1}.csv
	if [[ $to_use_pcp -eq 1 ]]; then
		metric_name="pyperf_${test_name}"
		ns_results=$($TOOLS_BIN/convert_val --from_unit $unit --to_unit ns --time_val --value $results | sed -e 's/ns//g')
		# Cannot use convert_val directly since it does not support floating point output
		sec_results=$(echo "$ns_results/1000000000" | bc -l)
		result2pcp "$metric_name" "${sec_results}"
	fi
}

pip3_install()
{
	if [ $to_no_pkg_install -eq 1 ]; then
		return
	fi

	$python_exec -m pip --version
	if [[ $? -ne 0 ]]; then
			if [[ $install_pip -eq 1 ]]; then
			$python_exec -m ensurepip || exit_out "Failed to install pip." 1
		else
			exit_out "Pip is not available, exiting out" 1
		fi
	fi

	$python_exec -m pip install -q $1
	if [[ $? -ne 0 ]]; then
		exit_out "Pip not available for install of $1 failed." 1
	fi
}
#
# Variables set by general setup.
#
# TOOLS_BIN: points to the tool directory
# to_home_root: home directory
# to_configuration: configuration information
# to_times_to_run: number of times to run the test
# to_user: User on the test system running the test
# to_sys_type: for results info, basically aws, azure or local
# to_sysname: name of the system
# to_tuned_setting: tuned setting
#

install_tools $0

test_name_run="pyperf"
arguments="$@"

curdir=`pwd`

if [[ $0 == "./"* ]]; then
	chars=`echo $0 | awk -v RS='/' 'END{print NR-1}'`
	if [[ $chars == 1 ]]; then
		run_dir=`pwd`
	else
		run_dir=`echo $0 | cut -d'/' -f 1-${chars} | cut -d'.' -f2-`
		run_dir="${curdir}${run_dir}"
	fi
elif [[ $0 != "/"* ]]; then
	dir=`echo $0 | rev | cut -d'/' -f2- | rev`
	run_dir="${curdir}/${dir}"
else
	chars=`echo $0 | awk -v RS='/' 'END{print NR-1}'`
	run_dir=`echo $0 | cut -d'/' -f 1-${chars}`
	if [[ $run_dir != "/"* ]]; then
		run_dir=${curdir}/${run_dir}
	fi
fi

# Gather hardware information
${curdir}/test_tools/gather_data ${curdir}


if [ ! -f "/tmp/pyperf.out" ]; then
        command="${0} $@"
        echo $command
        $command &> /tmp/pyperf.out
	rtc=$?
	if [ -f /tmp/pyperf.out ]; then
		echo =================================
		echo Output from the test.
		echo =================================
        	cat /tmp/pyperf.out
        	rm /tmp/pyperf.out
	fi
        exit $rtc
fi


if [ -d "workloads" ]; then
	#
	# If running from zathras, workloads will be symlinked to
	# to /mnt.  Which is done due to azure having a very small
	# user space.
	#
	start_dir=`pwd`
	cd workloads
	for file in `ls ${start_dir}`; do
		if [[ ! -f $file ]] && [[ ! -d $file ]]; then
			ln -s $start_dir/$file .
		fi
	done
fi

source test_tools/general_setup "$@"

ARGUMENT_LIST=(
	"pyperf_version"
	"python_exec"
	"python_pkgs"
)

NO_ARGUMENTS=(
	"usage"
	"install_pip"
)

# read arguments
opts=$(getopt \
	--longoptions "$(printf "%s:," "${ARGUMENT_LIST[@]}")" \
        --longoptions "$(printf "%s," "${NO_ARGUMENTS[@]}")" \
        --name "$(basename "$0")" \
        --options "h" \
        -- "$@"
)

eval set --$opts

while [[ $# -gt 0 ]]; do
	case "$1" in
		--pyperf_version)
			PYPERF_VERSION=$2
			shift 2
		;;
		--python_exec)
			python_exec=$2
			shift 2
		;;
		--python_pkgs)
			python_pkgs=$2
			shift 2
		;;
		--install_pip)
			install_pip=1
			shift 1
		;;
		--usage)
			usage $0
		;;
		-h)
			usage $0
		;;
		--)
			break
		;;
		*)
			echo option not found $1
			usage $0
		;;
	esac
done

PYTHON_VERSION=$($python_exec --version | awk '{ print $2 }')
if [[ ${python_pkgs} != "" ]]; then
	pkg_list="--packages `echo $python_pkgs | sed "s/,/ /g"`"
fi
if ! command -v $python_exec; then
	exit_out "Error: Designated python executable, $python_exec, not present"
fi

$TOOLS_BIN/package_tool --wrapper_config $base_dir/../pyperf.json $pkg_list --pip_packages "pyperformance==$PYPERF_VERSION"

cpus=`cat /proc/cpuinfo | grep processor | wc -l`
cous=1
mkdir python_results

pyresults=python_results/pyperf_out_$(date "+%Y.%m.%d-%H.%M.%S")

if [[ $to_use_pcp -eq 1 ]]; then
	source $TOOLS_BIN/pcp/pcp_commands.inc
	setup_pcp
	pcp_cfg=$TOOLS_BIN/pcp/default.cfg
	pcp_dir=/tmp/pcp_pyperf_$(date "+%Y.%m.%d-%H.%M.%S")
	start_pcp $pcp_dir/ pyperf $pcp_cfg
	start_pcp_subset
	copy_dirs="$pcp_dir"
fi

$python_exec -m pyperformance run --output  ${pyresults}.json
if [ $? -ne 0 ]; then
	exit_out "Failed: $python_exec -m pyperformance run --output  ${pyresults}.json" 1
fi

$python_exec -m pyperf dump  ${pyresults}.json > ${pyresults}.results
if [ $? -ne 0 ]; then
	echo "Failed: $python_exec -m pyperf dump  ${pyresults}.json > ${pyresults}.results" 1
	echo Failed > test_results_report
else
	echo Ran > test_results_report
fi

generate_csv_file ${pyresults}

if [[ $to_use_pcp -eq 1 ]]; then
	stop_pcp_subset
	stop_pcp
	shutdown_pcp
fi


if [[ ! -z "$copy_dirs" ]]; then
	copy_dirs="--copy_dir $copy_dirs"
fi

#
# Process the data.
#
${curdir}/test_tools/save_results --curdir $curdir --home_root $to_home_root --results /tmp/pyperf.out  --test_name pyperf --tuned_setting=$to_tuned_setting --version NONE --user $to_user --other_files "python_results/*,test_results_report" $copy_dirs
exit 0
