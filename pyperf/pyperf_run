#!/bin/bash
#    An automated script to run pyperformane using specified python binaries
#    Copyright (C) 2024 David Valin <dvalin@redhat.com>
#
#    This program is free software; you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation; either version 2 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License along
#    with this program; if not, see <https://www.gnu.org/licenses/>.

PATH="${PATH}:/usr/local/bin"
export PATH
python_pkgs=""
python_exec=""
PYPERF_VERSION="1.11.0"
#
# To make sure.
#
exit_out()
{
	echo $1
	exit $2
}

usage()
{
	echo "$1 Usage:"
	echo "--pyperf_version <version number>: Version of pyperf to run, default is $PYPERF_VERSION."
	echo "--python_exec_path: Python to set via alternatives"
	echo "--python_pkgs: comma seprated list of python packages to install"
	source test_tools/general_setup --usage
        exit 1
}

install_tools()
{
	show_usage=0
	#
	# Clone the repo that contains the common code and tools
	#
	tools_git=https://github.com/redhat-performance/test_tools-wrappers

	found=0
	for arg in "$@"; do
		if [ $found -eq 1 ]; then
			tools_git=$arg
			found=0
		fi
		if [[ $arg == "--tools_git" ]]; then
			found=1
		fi
		#
		# We do the usage check here, as we do not want to be calling
		# the common parsers then checking for usage here.  Doing so will
		# result in the script exiting with out giving the test options.
		#
		if [[ $arg == "--usage" ]]; then
			show_usage=1
		fi
	done

	#
	# Check to see if the test tools directory exists.  If it does, we do not need to
	# clone the repo.
	#
	if [ ! -d "test_tools" ]; then
		git clone $tools_git test_tools
		if [ $? -ne 0 ]; then
			exit_out "pulling git $tools_git failed." 1
		fi
	fi

	if [ $show_usage -eq 1 ]; then
		usage $1
	fi
}

generate_csv_file()
{
	instance=0
	float=0
	ivalue=0
	fvalue=0.0
	test_name=""
	unit=""
	reduce=0
	res_count=0
	value_sum=0

  $TOOLS_BIN/test_header_info --front_matter --results_file "${1}.csv" --host $to_configuration --sys_type $to_sys_type --tuned $to_tuned_setting --results_version $PYPERF_VERSION --test_name $test_name_run

	echo "Test:Avg:Unit" >> "${1}.csv"
	while IFS= read -r line
	do
		if [[ $test_name == "" ]]; then
			test_name=$line
			continue
		fi
		if [ -z "$line" ]; then
			let "reduce=$reduce+1"
			if [[ $reduce -eq 2 ]]; then
				if [[ $test_name != *"WARNING:"* ]]; then
					results=`echo "${value_sum}/${res_count}" | bc -l`
					printf "%s:%.2f:%s\n" $test_name $results $unit >> ${1}.csv
				fi
				reduce=0
				res_count=0
				value_sum=0
				test_name=""
			fi
			continue
		fi
		if [[ $line == *"--"* ]] || [[ $line == *"calibrate"* ]] || [[ $line == *"warmup"* ]] || [[ $line != *"value"* ]]; then
			continue
		fi
		value=`echo $line | cut -d' ' -f 4`
		unit=`echo $line | cut -d' ' -f 5`
		let "res_count=${res_count}+1"
		value_sum=`echo "${value}+${value_sum}" | bc -l`
	done < "${1}.results"
	results=`echo "${value_sum}/${res_count}" | bc -l`
	printf "%s:%.2f:%s\n" $test_name $results $unit >> ${1}.csv
}

pip3_install()
{
	if [ $to_no_pkg_install -eq 0 ]; then
		pip3 -q install $1
		if [ $? -ne 0 ]; then
			exit_out "pip3 install of $1 failed." 1
		fi
	fi
}
#
# Variables set by general setup.
#
# TOOLS_BIN: points to the tool directory
# to_home_root: home directory
# to_configuration: configuration information
# to_times_to_run: number of times to run the test
# to_pbench: Run the test via pbench
# to_pbench_copy: Copy the data to the pbench repository, not move_it.
# to_puser: User running pbench
# to_run_label: Label for the run
# to_user: User on the test system running the test
# to_sys_type: for results info, basically aws, azure or local
# to_sysname: name of the system
# to_tuned_setting: tuned setting
#

install_tools $0

test_name_run="pyperf"
arguments="$@"

curdir=`pwd`

if [[ $0 == "./"* ]]; then
	chars=`echo $0 | awk -v RS='/' 'END{print NR-1}'`
	if [[ $chars == 1 ]]; then
		run_dir=`pwd`
	else
		run_dir=`echo $0 | cut -d'/' -f 1-${chars} | cut -d'.' -f2-`
		run_dir="${curdir}${run_dir}"
	fi
elif [[ $0 != "/"* ]]; then
	dir=`echo $0 | rev | cut -d'/' -f2- | rev`
	run_dir="${curdir}/${dir}"
else
	chars=`echo $0 | awk -v RS='/' 'END{print NR-1}'`
	run_dir=`echo $0 | cut -d'/' -f 1-${chars}`
	if [[ $run_dir != "/"* ]]; then
		run_dir=${curdir}/${run_dir}
	fi
fi

# Gather hardware information
${curdir}/test_tools/gather_data ${curdir}


if [ ! -f "/tmp/pyperf.out" ]; then
        command="${0} $@"
        echo $command
        $command &> /tmp/pyperf.out
	rtc=$?
	if [ -f /tmp/pyperf.out ]; then
		echo =================================
		echo Output from the test.
		echo =================================
        	cat /tmp/pyperf.out
        	rm /tmp/pyperf.out
	fi
        exit $rtc
fi


if [ -d "workloads" ]; then
	#
	# If running from zathras, workloads will be symlinked to
	# to /mnt.  Which is done due to azure having a very small
	# user space.
	#
	start_dir=`pwd`
	cd workloads
	for file in `ls ${start_dir}`; do
		if [[ ! -f $file ]] && [[ ! -d $file ]]; then
			ln -s $start_dir/$file .
		fi
	done
fi

source test_tools/general_setup "$@"

ARGUMENT_LIST=(
	"pyperf_version"
	"python_exec"
        "python_pkgs"
)

NO_ARGUMENTS=(
        "usage"
)

# read arguments
opts=$(getopt \
	--longoptions "$(printf "%s:," "${ARGUMENT_LIST[@]}")" \
        --longoptions "$(printf "%s," "${NO_ARGUMENTS[@]}")" \
        --name "$(basename "$0")" \
        --options "h" \
        -- "$@"
)

eval set --$opts

while [[ $# -gt 0 ]]; do
	case "$1" in
		--pyperf_version)
			PYPERF_VERSION=$2
			shift 2
		;;
		--python_exec)
			python_exec=$2
			shift 2
		;;
		--python_pkgs)
			python_pkgs=$2
			shift 2
		;;
		--usage)
			usage $0
		;;
		-h)
			usage $0
		;;
		--)
			break
		;;
		*)
			echo option not found $1
			usage $0
		;;
	esac
done

if [ $to_pbench -eq 0 ]; then
	rm -rf pyperformance
	python3 --version
	python3 -m pip install pyperformance==$PYPERF_VERSION
	if [ $? -ne 0 ]; then
		exit_out "python3 -m pip install pyperformance==$PYPERF_VERSION: failed" 1
	fi
	cd pyperformance
	if [[ ${python_pkgs} != "" ]]; then
		pkg_list=`echo $python_pkgs | sed "s/,/ /g"`
		test_tools/package_install --packages "$python_pkgs" --no_packages $to_no_pkg_install
	fi
	if [[ $python_exec != "" ]]; then
		if [[ ! -f $python_exec ]]; then
			exit_out "Error: Designated python executable, $python_exec, not present"
		fi
		#
		# Remove the existing (if any) default python.
		#
		alternatives --remove-all python
 		alternatives --install /usr/bin/python python $python_exec 1
	fi
	pip3_install psutil
	pip3_install packaging
	pip3_install pyparsing
	pip3_install pyperf
	pip3_install toml

	cpus=`cat /proc/cpuinfo | grep processor | wc -l`
	cous=1
	mkdir python_results
	
	pyresults=python_results/pyperf_out_$(date "+%Y.%m.%d-%H.%M.%S")
	pwd > /tmp/dave_debug
	echo python3 -m pyperformance run --output  ${pyresults}.json >> /tmp/dave_debug
	python3 -m pyperformance run --output  ${pyresults}.json
	if [ $? -ne 0 ]; then
		exit_out "Failed: python3 -m pyperformance run --output  ${pyresults}.json" 1
	fi
	echo python3 -m pyperf dump  ${pyresults}.json >> /tmp/dave_debug
	python3 -m pyperf dump  ${pyresults}.json > ${pyresults}.results
	if [ $? -ne 0 ]; then
		echo "Failed: python3 -m pyperf dump  ${pyresults}.json > ${pyresults}.results" 1
		echo Failed > test_results_report
	else
		echo Ran > test_results_report
	fi

	generate_csv_file ${pyresults}
else
	source ~/.bashrc
	arguments="${arguments} --test_iterations ${to_times_to_run}"
	cd $curdir
	echo $TOOLS_BIN/execute_via_pbench --cmd_executing "$0" $arguments --test ${test_name_run} --spacing 11 --pbench_stats $to_pstats
	$TOOLS_BIN/execute_via_pbench --cmd_executing "$0" $arguments --test ${test_name_run} --spacing 11 --pbench_stats $to_pstats
	if [ $? -ne 0  ]; then
		exit_out "Failed: $TOOLS_BIN/execute_via_pbench --cmd_executing "$0" $arguments --test ${test_name_run} --spacing 11 --pbench_stats $to_pstats"
	fi
	exit 0
fi


#
# Process the data.
#
${curdir}/test_tools/save_results --curdir $curdir --home_root $to_home_root --results /tmp/pyperf.out  --test_name pyperf --tuned_setting=$to_tuned_setting --version NONE --user $to_user --other_files "python_results/*,test_results_report"
exit 0
